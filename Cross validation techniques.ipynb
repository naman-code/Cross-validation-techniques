{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validations is used to stir up your data and find what can be your best and worst accuracy for that data set.\n",
    "how it is done is it splits the trainig and testing data differently every time so everytime your model is trained and tested and different set of value that leads to different accuracies."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import padas as pd\n",
    "data = pd.read_csv(\"File_name.csv\")\n",
    "x = data.drop[\"target_col\"]\n",
    "y = data.target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are different types of cross validation techniques :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or all techniques the model i have used is decision tree we can change the model to any of our choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) HoldOut Validation Approach- Train And Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the conventional way to finding the accuracy in this we normnally use train test split from sklearn library and and put different values of random state to get different accuracies nothing impressive So we have better methods as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a better method in which when we initailise the object of it we pass no if iterations we need to to \n",
    "(ie if we pass 10 them we will do 10 number of cross validatios in our data )\n",
    "In this for 10 validations we break our data into 10 equal parts and we use 1 part for test and 9 parts for train for every validation( for eg if we have 100 data points and we break into 10 points gropu we will have 10 groups so for 1st validation 1 to 10 will be test set and 10 to 100 will be rain set ,, for 2nd validation 10 to 20 will be test set and 1 to 10 + 20 to 100 qwill be train set and this goes onn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "model=DecisionTreeClassifier()\n",
    "kfold_validation=KFold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "results=cross_val_score(model,X,y,cv=kfold_validation)\n",
    "print(results)\n",
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stratified K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the the best validation approach for imbalanced data set where one output category is outnumbered by other output category.\n",
    "What happens in this i the test data that we take contain equally no of output values(ie if 20 data point is taken for test set \n",
    "then 10 will be from 1st output category and 10 will be from other category) to see how good model works on both category values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skfold=StratifiedKFold(n_splits=5)\n",
    "model=DecisionTreeClassifier()\n",
    "scores=cross_val_score(model,X,y,cv=skfold)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Leave One Out Cross Validation(LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one out is another one of validation techniques in which we use 1 data point for test and remaining for training the\n",
    "model. this approach is not used anymore as it is very time consuming as for 1000 data points 1000 validation will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "model=DecisionTreeClassifier()\n",
    "leave_validation=LeaveOneOut()\n",
    "results=cross_val_score(model,X,y,cv=leave_validation)\n",
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. \n",
    "Repeated Random Test-Train Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is a hybrid of traditional train-test splitting and the k-fold cross-validation method. In this technique, we create random splits of the data in the training-test set manner and then repeat the process of splitting and evaluating the algorithm multiple times, just like the cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "model=DecisionTreeClassifier()\n",
    "ssplit=ShuffleSplit(n_splits=10,test_size=0.30)\n",
    "results=cross_val_score(model,X,y,cv=ssplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "print(\n",
    "np.mean(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
